import os
import glob
from lxml import etree
import multiprocessing
from itertools import islice
import json

# ë¦¬ìŠ¤íŠ¸ë¥¼ hashmap ìœ¼ë¡œ ë³€í™˜
def list_to_map(data_list):
  return {item['url']: item for item in data_list}

# âœ… ê¸€ë¡œë²Œ ë¹„êµ í•¨ìˆ˜
def global_diff_update(old_list, new_list):
  """
  old_listì™€ new_listë¥¼ ê¸€ë¡œë²Œ HashMapì„ ì‚¬ìš©í•˜ì—¬ ë¹„êµí•©ë‹ˆë‹¤.
  """
  old_map = list_to_map(old_list)
  new_map = list_to_map(new_list)

  # âœ… ì¶”ê°€ëœ í•­ëª© (new_mapì—ë§Œ ì¡´ì¬)
  added = [item for url, item in new_map.items() if url not in old_map]
  
  # âœ… ì‚­ì œëœ í•­ëª© (old_mapì—ë§Œ ì¡´ì¬)
  removed = [url for url in old_map if url not in new_map]
  
  # âœ… ë³€ê²½ë˜ì§€ ì•Šì€ í•­ëª© (ë‘ ë§µì— ëª¨ë‘ ì¡´ì¬)
  unchanged = [item for url, item in new_map.items() if url in old_map]
  
  # âœ… ìµœì¢… ë¦¬ìŠ¤íŠ¸: ìœ ì§€ëœ í•­ëª© + ì¶”ê°€ëœ í•­ëª©
  final_list = unchanged + added
  
  return final_list, removed
  
def compare_batches(old_map, new_batch):
  """
  ê° ë°°ì¹˜ë¥¼ old_mapê³¼ ë¹„êµí•˜ì—¬ added, removed, unchangedë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
  """
  new_map = {item['url']: item for item in new_batch}
  added = [item for url, item in new_map.items() if url not in old_map]
  removed = [url for url in old_map if url not in new_map]
  unchanged = [item for url, item in new_map.items() if url in old_map]
  return {'added': added, 'removed': removed, 'unchanged': unchanged}

# âœ… ë©€í‹°í”„ë¡œì„¸ì‹±ì„ í™œìš©í•œ ê¸€ë¡œë²Œ ë¹„êµ
def multiprocessing_global_diff(old_list, new_list, num_processes=4):
  """
  ê¸€ë¡œë²Œ HashMapì„ ê¸°ë°˜ìœ¼ë¡œ ë©€í‹°í”„ë¡œì„¸ì‹±ì„ í†µí•´ old_listì™€ new_listë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
  """
  old_map = list_to_map(old_list)
  batch_size = len(new_list) // num_processes + 1
  new_batches = [new_list[i:i + batch_size] for i in range(0, len(new_list), batch_size)]

  with multiprocessing.Pool(num_processes) as pool:
    results = pool.starmap(compare_batches, [(old_map, batch) for batch in new_batches])
  
  # ê²°ê³¼ ë³‘í•©
  final_added = []
  final_removed = []
  final_unchanged = []
  
  for result in results:
    final_added.extend(result['added'])
    final_removed.extend(result['removed'])
    final_unchanged.extend(result['unchanged'])
  
  final_list = final_unchanged + final_added
  
  return final_list, final_removed

# âœ… HTML íŒŒì¼ì—ì„œ URLê³¼ ì œëª©ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ (XPath ì‚¬ìš©)
def extract_links_from_html(file_path):
  """
  XPathë¥¼ ì‚¬ìš©í•˜ì—¬ HTML íŒŒì¼ì—ì„œ <body> â†’ <section> â†’ <ul> â†’ <li> â†’ <a> êµ¬ì¡°ë¡œ URLê³¼ ì œëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
  """
  links = []  # ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    
  # HTML íŒŒì¼ ì½ê¸°
  with open(file_path, 'r', encoding='utf-8') as file:
    content = file.read()
    parser = etree.HTMLParser()
    tree = etree.fromstring(content, parser)  # lxmlì„ ì‚¬ìš©í•´ HTML íŒŒì‹±

    # XPathë¥¼ ì‚¬ìš©í•´ a íƒœê·¸ ì°¾ê¸°
    li_tags = tree.xpath('//body/section/ul/li')
    
    for li in li_tags:
      a_tag = li.xpath('./a[@class="h-cite"]')
      time_tag = li.xpath('./time[@class="dt-published"]')

      url = a_tag[0].get('href') if a_tag else None  # href ì†ì„± ì¶”ì¶œ
      title = a_tag[0].text.strip() if a_tag and a_tag[0].text else None  # íƒœê·¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê³µë°± ì œê±°)
      time = time_tag[0].text.strip() if time_tag and time_tag[0].text else None
      
      if url and title:
        links.append({'url': url, 'title': title, 'time': time})
    
  return links

# âœ… ì—¬ëŸ¬ HTML íŒŒì¼ì—ì„œ URLê³¼ ì œëª©ì„ ì¶”ì¶œ
def process_multiple_html_files(directory, file_pattern='bookmarks-*.html'):
  """
  ì£¼ì–´ì§„ ë””ë ‰í„°ë¦¬ì—ì„œ ì—¬ëŸ¬ HTML íŒŒì¼ì„ ì½ê³  URLê³¼ ì œëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
  """
  all_links = []  # ëª¨ë“  ë§í¬ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
  
  # íŒŒì¼ íŒ¨í„´ì— ë§ëŠ” ëª¨ë“  HTML íŒŒì¼ ì°¾ê¸°
  file_paths = glob.glob(os.path.join(directory, file_pattern))
  
  for file_path in file_paths:
    print(f"ğŸ“„ Processing: {file_path}")
    links = extract_links_from_html(file_path)
    all_links.extend(links)  # ì¶”ì¶œëœ ë§í¬ë¥¼ ì „ì²´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
  
  return all_links

# âœ… JSON íŒŒì¼ì—ì„œ ê¸°ì¡´ ë¦¬ìŠ¤íŠ¸(old_list) ë¶ˆëŸ¬ì˜¤ê¸°
def load_old_list(json_file='output_links.json'):
  """
  ê¸°ì¡´ JSON íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ old_listë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
  """
  if os.path.exists(json_file):
    with open(json_file, 'r', encoding='utf-8') as file:
      old_list = json.load(file)
      print(f"âœ… Loaded {len(old_list)} items from {json_file}")
      return old_list
  else:
    print(f"âš ï¸ No existing JSON file found at {json_file}. Returning an empty list.")
    return []

# âœ… ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
def save_to_json(data, output_file='output_links.json'):
  """
  ì¶”ì¶œëœ URL, ì œëª©, ì‹œê°„ì„ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
  """
  with open(output_file, 'w', encoding='utf-8') as jsonfile:
    json.dump(data, jsonfile, indent=4, ensure_ascii=False)
  
  print(f"âœ… Data saved to {output_file}")

# âœ… ë©”ì¸ ì‹¤í–‰
if __name__ == '__main__':
  # HTML íŒŒì¼ë“¤ì´ ì €ì¥ëœ ë””ë ‰í„°ë¦¬ ê²½ë¡œ ì„¤ì •
  input_directory = './bookmarks'  # í˜„ì¬ ë””ë ‰í„°ë¦¬
  
  old_links = load_old_list()
  
  # ê¸°ì¡´ ë¦¬ìŠ¤íŠ¸ ê²°ê³¼ ì¶œë ¥ (ì¼ë¶€ë§Œ í™•ì¸)
  for link in old_links[:5]:
    print(link)
  
  # ì—¬ëŸ¬ HTML íŒŒì¼ ì²˜ë¦¬
  new_links = process_multiple_html_files(input_directory)
  
  # ìƒˆë¡œìš´ ë¦¬ìŠ¤íŠ¸ ê²°ê³¼ ì¶œë ¥ (ì¼ë¶€ë§Œ í™•ì¸)
  for link in new_links[:5]:
    print(link)
  
  # updated_list, removed_items = multiprocessing_global_diff(old_links, new_links)
  updated_list, removed_items = global_diff_update(old_links, new_links)

  print(len(updated_list))
  print(len(removed_items))

  print("âœ… ì—…ë°ì´íŠ¸ëœ ë¦¬ìŠ¤íŠ¸ (ì¼ë¶€):", updated_list[:5])
  print("âŒ ì‚­ì œëœ í•­ëª© (ì¼ë¶€):", removed_items[:5])